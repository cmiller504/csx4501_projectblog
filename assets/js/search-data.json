{
  
    
        "post0": {
            "title": "Environmental predictors of global food supply",
            "content": "Introduction . Climate change is predicted to have major impacts on crop production given that crop yields are dependent on environmental factors such as temperature, precipitation, and carbon dioxide (CO2). Understanding worldwide crop yield is central to addressing food security challenges and reducing the impacts of climate change. Thus, the goals of this project are to (i) create a model of crop yield based on relevant predictor variables and (ii) determine which features are the most important in predicting crop yield. . Methods . Data . I compiled a dataset with 28,235 records for the following variables: crop yield (hg/ha), country, year, rainfall (mm/year), average temperature, pesticide usage (tonnes), and CO2 concentrations (ppm).Data was collected from Kaggle (crop yield: https://www.kaggle.com/datasets/patelris/crop-yield-prediction-dataset; CO2: https://www.kaggle.com/datasets/yoannboyere/co2-ghg-emissionsdata) where crop yield and pesticides were sourced from The Food and Agriculture Organization of the United Nations (FAO, http://www.fao.org/home/en/), rainfall and temperature were sourced from World Bank Data (https://data.worldbank.org/), CO2 was sourced from OurWorldInData (https://ourworldindata.org/co2-and-other-greenhouse-gas-emissions). This dataset includes the topmost consumed crops worldwide: cassava, maize, potatoes, rice (paddy), sorghum, soybeans, sweet potatoes, wheat, yams, and plantains and others. There are records for 101 different countries for years 1990 through 2013. . EDA . 1. Importing . import pandas as pd import numpy as np . Load in all of the datasets we will use . yielddf = pd.read_csv(&#39;data/yield_df.csv&#39;) co2df = pd.read_csv(&#39;data/owid-co2-data.csv&#39;) . Data: Crop yield . yielddf.shape . (28242, 8) . yielddf . Unnamed: 0 Area Item Year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp . 0 0 | Albania | Maize | 1990 | 36613 | 1485.0 | 121.00 | 16.37 | . 1 1 | Albania | Potatoes | 1990 | 66667 | 1485.0 | 121.00 | 16.37 | . 2 2 | Albania | Rice, paddy | 1990 | 23333 | 1485.0 | 121.00 | 16.37 | . 3 3 | Albania | Sorghum | 1990 | 12500 | 1485.0 | 121.00 | 16.37 | . 4 4 | Albania | Soybeans | 1990 | 7000 | 1485.0 | 121.00 | 16.37 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 28237 28237 | Zimbabwe | Rice, paddy | 2013 | 22581 | 657.0 | 2550.07 | 19.76 | . 28238 28238 | Zimbabwe | Sorghum | 2013 | 3066 | 657.0 | 2550.07 | 19.76 | . 28239 28239 | Zimbabwe | Soybeans | 2013 | 13142 | 657.0 | 2550.07 | 19.76 | . 28240 28240 | Zimbabwe | Sweet potatoes | 2013 | 22222 | 657.0 | 2550.07 | 19.76 | . 28241 28241 | Zimbabwe | Wheat | 2013 | 22888 | 657.0 | 2550.07 | 19.76 | . 28242 rows × 8 columns . yielddf.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 28242 entries, 0 to 28241 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 Unnamed: 0 28242 non-null int64 1 Area 28242 non-null object 2 Item 28242 non-null object 3 Year 28242 non-null int64 4 hg/ha_yield 28242 non-null int64 5 average_rain_fall_mm_per_year 28242 non-null float64 6 pesticides_tonnes 28242 non-null float64 7 avg_temp 28242 non-null float64 dtypes: float64(3), int64(3), object(2) memory usage: 1.7+ MB . yielddf.describe(include = &#39;all&#39;) . Unnamed: 0 Area Item Year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp . count 28242.000000 | 28242 | 28242 | 28242.000000 | 28242.000000 | 28242.00000 | 28242.000000 | 28242.000000 | . unique NaN | 101 | 10 | NaN | NaN | NaN | NaN | NaN | . top NaN | India | Potatoes | NaN | NaN | NaN | NaN | NaN | . freq NaN | 4048 | 4276 | NaN | NaN | NaN | NaN | NaN | . mean 14120.500000 | NaN | NaN | 2001.544296 | 77053.332094 | 1149.05598 | 37076.909344 | 20.542627 | . std 8152.907488 | NaN | NaN | 7.051905 | 84956.612897 | 709.81215 | 59958.784665 | 6.312051 | . min 0.000000 | NaN | NaN | 1990.000000 | 50.000000 | 51.00000 | 0.040000 | 1.300000 | . 25% 7060.250000 | NaN | NaN | 1995.000000 | 19919.250000 | 593.00000 | 1702.000000 | 16.702500 | . 50% 14120.500000 | NaN | NaN | 2001.000000 | 38295.000000 | 1083.00000 | 17529.440000 | 21.510000 | . 75% 21180.750000 | NaN | NaN | 2008.000000 | 104676.750000 | 1668.00000 | 48687.880000 | 26.000000 | . max 28241.000000 | NaN | NaN | 2013.000000 | 501412.000000 | 3240.00000 | 367778.000000 | 30.650000 | . Notes: . over 28k data records | crop yield data from 101 countries from years 1990 - 2013 | Large range in crop yield values (50 hg/ha to over 501k hg/ha) | Avg rainfall ranges from 51 mm/year to 3240 | Pesticides ranges from 0.04 tonnes to over 367k | Temperature ranges from 1.3 to 30.65 &#39;C (just above freezing to 87&#39;F) | Potatoes are the most common crop | We have the most data from India | . Data : CO2 . co2df.shape . (25989, 60) . co2df . iso_code country year co2 co2_per_capita trade_co2 cement_co2 cement_co2_per_capita coal_co2 coal_co2_per_capita ... ghg_excluding_lucf_per_capita methane methane_per_capita nitrous_oxide nitrous_oxide_per_capita population gdp primary_energy_consumption energy_per_capita energy_per_gdp . 0 AFG | Afghanistan | 1949 | 0.015 | 0.002 | NaN | NaN | NaN | 0.015 | 0.002 | ... | NaN | NaN | NaN | NaN | NaN | 7624058.0 | NaN | NaN | NaN | NaN | . 1 AFG | Afghanistan | 1950 | 0.084 | 0.011 | NaN | NaN | NaN | 0.021 | 0.003 | ... | NaN | NaN | NaN | NaN | NaN | 7752117.0 | 9.421400e+09 | NaN | NaN | NaN | . 2 AFG | Afghanistan | 1951 | 0.092 | 0.012 | NaN | NaN | NaN | 0.026 | 0.003 | ... | NaN | NaN | NaN | NaN | NaN | 7840151.0 | 9.692280e+09 | NaN | NaN | NaN | . 3 AFG | Afghanistan | 1952 | 0.092 | 0.012 | NaN | NaN | NaN | 0.032 | 0.004 | ... | NaN | NaN | NaN | NaN | NaN | 7935996.0 | 1.001733e+10 | NaN | NaN | NaN | . 4 AFG | Afghanistan | 1953 | 0.106 | 0.013 | NaN | NaN | NaN | 0.038 | 0.005 | ... | NaN | NaN | NaN | NaN | NaN | 8039684.0 | 1.063052e+10 | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25984 ZWE | Zimbabwe | 2016 | 10.738 | 0.765 | 1.415 | 0.639 | 0.046 | 6.959 | 0.496 | ... | 2.076 | 11.50 | 0.820 | 6.21 | 0.443 | 14030338.0 | 2.096179e+10 | 46.666 | 3326.073 | 2.226 | . 25985 ZWE | Zimbabwe | 2017 | 9.582 | 0.673 | 1.666 | 0.678 | 0.048 | 5.665 | 0.398 | ... | 2.023 | 11.62 | 0.816 | 6.35 | 0.446 | 14236599.0 | 2.194784e+10 | 45.936 | 3226.617 | 2.093 | . 25986 ZWE | Zimbabwe | 2018 | 11.854 | 0.821 | 1.308 | 0.697 | 0.048 | 7.101 | 0.492 | ... | 2.173 | 11.96 | 0.828 | 6.59 | 0.456 | 14438812.0 | 2.271535e+10 | 47.502 | 3289.887 | 2.091 | . 25987 ZWE | Zimbabwe | 2019 | 10.949 | 0.748 | 1.473 | 0.697 | 0.048 | 6.020 | 0.411 | ... | NaN | NaN | NaN | NaN | NaN | 14645473.0 | NaN | 49.427 | 3374.877 | NaN | . 25988 ZWE | Zimbabwe | 2020 | 10.531 | 0.709 | NaN | 0.697 | 0.047 | 6.257 | 0.421 | ... | NaN | NaN | NaN | NaN | NaN | 14862927.0 | NaN | NaN | NaN | NaN | . 25989 rows × 60 columns . co2df.describe(include = &#39;all&#39;) . iso_code country year co2 co2_per_capita trade_co2 cement_co2 cement_co2_per_capita coal_co2 coal_co2_per_capita ... ghg_excluding_lucf_per_capita methane methane_per_capita nitrous_oxide nitrous_oxide_per_capita population gdp primary_energy_consumption energy_per_capita energy_per_gdp . count 21960 | 25989 | 25989.000000 | 24670.000000 | 24032.000000 | 4096.000000 | 12668.000000 | 12638.000000 | 17909.000000 | 17536.000000 | ... | 5651.000000 | 5655.000000 | 5655.000000 | 5655.000000 | 5655.000000 | 2.289200e+04 | 1.346900e+04 | 9345.000000 | 9300.000000 | 7149.000000 | . unique 219 | 248 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . top GBR | United Kingdom | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . freq 271 | 271 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . mean NaN | NaN | 1952.168225 | 326.658348 | 4.115845 | -8.124470 | 15.853638 | 0.111586 | 212.296783 | 1.543364 | ... | 6.870665 | 79.072368 | 1.902152 | 29.088635 | 0.601973 | 7.068041e+07 | 2.889570e+11 | 1425.651569 | 24469.973841 | 1.805888 | . std NaN | NaN | 54.592939 | 1677.027130 | 14.700552 | 262.090605 | 84.179826 | 0.147534 | 863.287148 | 2.524852 | ... | 7.270324 | 545.014528 | 3.318293 | 198.147748 | 0.890161 | 3.794737e+08 | 2.185600e+12 | 8750.538123 | 32096.347648 | 1.779325 | . min NaN | NaN | 1750.000000 | 0.000000 | 0.000000 | -2232.999000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.101000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.490000e+03 | 5.543200e+07 | 0.000000 | 0.000000 | 0.078000 | . 25% NaN | NaN | 1922.000000 | 0.557000 | 0.253000 | -1.663750 | 0.134000 | 0.019000 | 0.359000 | 0.056000 | ... | 2.095500 | 2.005000 | 0.691000 | 0.510000 | 0.221000 | 1.287425e+06 | 9.790110e+09 | 6.440000 | 3067.494750 | 0.859000 | . 50% NaN | NaN | 1966.000000 | 5.333000 | 1.226000 | 1.874500 | 0.603000 | 0.068000 | 4.540000 | 0.441500 | ... | 4.442000 | 8.530000 | 1.077000 | 3.460000 | 0.377000 | 4.869928e+06 | 3.044678e+10 | 54.352000 | 12983.272000 | 1.309000 | . 75% NaN | NaN | 1994.000000 | 48.153250 | 4.612250 | 9.700500 | 3.255000 | 0.155000 | 42.086000 | 2.141000 | ... | 8.975000 | 30.025000 | 1.619000 | 11.195000 | 0.589000 | 1.757624e+07 | 1.274622e+11 | 326.727000 | 34081.015500 | 2.203000 | . max NaN | NaN | 2020.000000 | 36702.503000 | 748.639000 | 2047.575000 | 1626.371000 | 2.738000 | 15062.902000 | 34.184000 | ... | 53.650000 | 8298.270000 | 39.795000 | 3078.270000 | 10.056000 | 7.794799e+09 | 1.136302e+14 | 161530.754000 | 308704.252000 | 25.253000 | . 11 rows × 60 columns . Notes: . over 25k data records | years 1750 - 2020 | 248 countries | CO2 ranges from 0 to over 36k parts per million (ppm) - (&#39;fresh air’ has a CO2 level between 400 to 450ppm) | . 2. Cleaning, organizing, and merging . yielddf.drop([&#39;Unnamed: 0&#39;],axis=1,inplace=True) yielddf . Area Item Year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp . 0 Albania | Maize | 1990 | 36613 | 1485.0 | 121.00 | 16.37 | . 1 Albania | Potatoes | 1990 | 66667 | 1485.0 | 121.00 | 16.37 | . 2 Albania | Rice, paddy | 1990 | 23333 | 1485.0 | 121.00 | 16.37 | . 3 Albania | Sorghum | 1990 | 12500 | 1485.0 | 121.00 | 16.37 | . 4 Albania | Soybeans | 1990 | 7000 | 1485.0 | 121.00 | 16.37 | . ... ... | ... | ... | ... | ... | ... | ... | . 28237 Zimbabwe | Rice, paddy | 2013 | 22581 | 657.0 | 2550.07 | 19.76 | . 28238 Zimbabwe | Sorghum | 2013 | 3066 | 657.0 | 2550.07 | 19.76 | . 28239 Zimbabwe | Soybeans | 2013 | 13142 | 657.0 | 2550.07 | 19.76 | . 28240 Zimbabwe | Sweet potatoes | 2013 | 22222 | 657.0 | 2550.07 | 19.76 | . 28241 Zimbabwe | Wheat | 2013 | 22888 | 657.0 | 2550.07 | 19.76 | . 28242 rows × 7 columns . co2df = co2df[[&#39;year&#39;,&#39;country&#39;, &#39;co2&#39;]] . yielddf.rename(columns={&quot;Area&quot;: &quot;country&quot;}, inplace = True) yielddf.rename(columns={&quot;Year&quot;: &quot;year&quot;}, inplace = True) yielddf.rename(columns={&quot;Item&quot;: &quot;crop&quot;}, inplace = True) . co2df.describe(include=&quot;all&quot;) . year country co2 . count 25989.000000 | 25989 | 24670.000000 | . unique NaN | 248 | NaN | . top NaN | United Kingdom | NaN | . freq NaN | 271 | NaN | . mean 1952.168225 | NaN | 326.658348 | . std 54.592939 | NaN | 1677.027130 | . min 1750.000000 | NaN | 0.000000 | . 25% 1922.000000 | NaN | 0.557000 | . 50% 1966.000000 | NaN | 5.333000 | . 75% 1994.000000 | NaN | 48.153250 | . max 2020.000000 | NaN | 36702.503000 | . df = pd.merge(yielddf, co2df, on=[&#39;year&#39;,&#39;country&#39;]) . df.describe(include = &#39;all&#39;) . country crop year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 . count 28242 | 28242 | 28242.000000 | 28242.000000 | 28242.00000 | 28242.000000 | 28242.000000 | 28235.000000 | . unique 101 | 10 | NaN | NaN | NaN | NaN | NaN | NaN | . top India | Potatoes | NaN | NaN | NaN | NaN | NaN | NaN | . freq 4048 | 4276 | NaN | NaN | NaN | NaN | NaN | NaN | . mean NaN | NaN | 2001.544296 | 77053.332094 | 1149.05598 | 37076.909344 | 20.542627 | 352.705764 | . std NaN | NaN | 7.051905 | 84956.612897 | 709.81215 | 59958.784665 | 6.312051 | 441.380634 | . min NaN | NaN | 1990.000000 | 50.000000 | 51.00000 | 0.040000 | 1.300000 | 0.117000 | . 25% NaN | NaN | 1995.000000 | 19919.250000 | 593.00000 | 1702.000000 | 16.702500 | 19.993000 | . 50% NaN | NaN | 2001.000000 | 38295.000000 | 1083.00000 | 17529.440000 | 21.510000 | 208.718000 | . 75% NaN | NaN | 2008.000000 | 104676.750000 | 1668.00000 | 48687.880000 | 26.000000 | 463.993000 | . max NaN | NaN | 2013.000000 | 501412.000000 | 3240.00000 | 367778.000000 | 30.650000 | 2036.937000 | . df.isnull().sum() . country 0 crop 0 year 0 hg/ha_yield 0 average_rain_fall_mm_per_year 0 pesticides_tonnes 0 avg_temp 0 co2 7 dtype: int64 . nan_rows = df[df.isna().any(axis=1)] nan_rows . country crop year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 . 8585 Eritrea | Maize | 1993 | 2308 | 383.0 | 4.44 | 24.09 | NaN | . 8586 Eritrea | Potatoes | 1993 | 82000 | 383.0 | 4.44 | 24.09 | NaN | . 8587 Eritrea | Sorghum | 1993 | 3197 | 383.0 | 4.44 | 24.09 | NaN | . 8588 Eritrea | Wheat | 1993 | 2038 | 383.0 | 4.44 | 24.09 | NaN | . 20954 Namibia | Maize | 1990 | 14871 | 285.0 | 42.00 | 20.40 | NaN | . 20955 Namibia | Sorghum | 1990 | 2052 | 285.0 | 42.00 | 20.40 | NaN | . 20956 Namibia | Wheat | 1990 | 44250 | 285.0 | 42.00 | 20.40 | NaN | . df = df.dropna() . df.describe(include = &#39;all&#39;) . country crop year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 . count 28235 | 28235 | 28235.000000 | 28235.000000 | 28235.000000 | 28235.000000 | 28235.000000 | 28235.000000 | . unique 101 | 10 | NaN | NaN | NaN | NaN | NaN | NaN | . top India | Potatoes | NaN | NaN | NaN | NaN | NaN | NaN | . freq 4048 | 4275 | NaN | NaN | NaN | NaN | NaN | NaN | . mean NaN | NaN | 2001.546733 | 77067.097184 | 1149.256313 | 37086.096332 | 20.542139 | 352.705764 | . std NaN | NaN | 7.051042 | 84961.460609 | 709.785667 | 59963.377495 | 6.312692 | 441.380634 | . min NaN | NaN | 1990.000000 | 50.000000 | 51.000000 | 0.040000 | 1.300000 | 0.117000 | . 25% NaN | NaN | 1995.000000 | 19928.000000 | 593.000000 | 1702.000000 | 16.700000 | 19.993000 | . 50% NaN | NaN | 2001.000000 | 38295.000000 | 1083.000000 | 17529.440000 | 21.510000 | 208.718000 | . 75% NaN | NaN | 2008.000000 | 104716.000000 | 1668.000000 | 48715.510000 | 26.000000 | 463.993000 | . max NaN | NaN | 2013.000000 | 501412.000000 | 3240.000000 | 367778.000000 | 30.650000 | 2036.937000 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 28235 entries, 0 to 28241 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 country 28235 non-null object 1 crop 28235 non-null object 2 year 28235 non-null int64 3 hg/ha_yield 28235 non-null int64 4 average_rain_fall_mm_per_year 28235 non-null float64 5 pesticides_tonnes 28235 non-null float64 6 avg_temp 28235 non-null float64 7 co2 28235 non-null float64 dtypes: float64(4), int64(2), object(2) memory usage: 1.9+ MB . df[&#39;year&#39;] = df[&#39;year&#39;].astype(str) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 28235 entries, 0 to 28241 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 country 28235 non-null object 1 crop 28235 non-null object 2 year 28235 non-null object 3 hg/ha_yield 28235 non-null int64 4 average_rain_fall_mm_per_year 28235 non-null float64 5 pesticides_tonnes 28235 non-null float64 6 avg_temp 28235 non-null float64 7 co2 28235 non-null float64 dtypes: float64(4), int64(1), object(3) memory usage: 1.9+ MB . Analyses . First, to explore potential linear relationships, I tested for correlations between pairs of numerical variables using the Pearson correlation coefficient (r). Next, I considered three supervised machine learning regression algorithms to predict the continuous-valued attribute of crop yield based on the values of several predictor variables: decision tree regression, random forest regression, and histogram-based gradient boosting regression. The histogram-based gradient boosting regression was used instead of a standard gradient boosting regression, because it is recommended for larger datasets (number of samples greater than 10,000). These regression tree and ensemble methods are generally advantageous for finding complex non-linear relationships in data. Since these are non-parametric approaches, they do not require normalized data. . Before applying these machine learning algorithms, data need to be preprocessed by encoding the categorical variables (crop type, year, and country) into numerical features. To do so, I used the scikitlearn OneHotEncoder function which reconfigures categorical features into binary features by “one-hot” encoding, such that if a feature is represented by a column, in that it receives a 1, otherwise, it receives a 0. Data was then split into training and testing sets, 80% and 20% respectively, in order to evaluate model performance. Model performance of the three algorithms were compared by calculating their R² value (the coefficient of determination), which is a statistical measure that determines the proportion of variance in the response variable that can be explained by the predictor variables. This tells us how well the data fit for each of the regression models (the goodness of fit). The random forest regression had the highest R2 of 98.6%, while decision tree regression and histogram gradient boosting regression had 97.6% and 95.5%, respectively. While these are all high R2 values, I selected the random forest regression for the final model. Once the best performing algorithm was determined, I evaluated a range of tree depths between 1 and 30 to determine the appropriate depth to minimize the root mean squared error while also avoiding overfitting the model. More specifically, I graphed the range of depths against their corresponding model’s root mean squared error and selected the value of depth that corresponds to the point in the curve of graphed points where the value of root mean squared error levels out. The model was fit with a depth of 25. I evaluated with model with the following metrics. Finally, I used the model to determine the top five most important variables in predicting crop yield. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. . 3. Plots and basic stats . Any correlations? . df.select_dtypes(include=[np.number]).corr() . hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 . hg/ha_yield 1.000000 | 0.000778 | 0.063992 | -0.114738 | 0.147793 | . average_rain_fall_mm_per_year 0.000778 | 1.000000 | 0.180847 | 0.313180 | -0.024053 | . pesticides_tonnes 0.063992 | 0.180847 | 1.000000 | 0.030996 | 0.270587 | . avg_temp -0.114738 | 0.313180 | 0.030996 | 1.000000 | 0.088063 | . co2 0.147793 | -0.024053 | 0.270587 | 0.088063 | 1.000000 | . import matplotlib.pyplot as plt import seaborn as sns correlation_data=df.select_dtypes(include=[np.number]).corr() mask = np.zeros_like(correlation_data) mask[np.triu_indices_from(mask)] = True f, ax = plt.subplots(figsize=(11, 9)) # Generate a custom diverging colormap cmap = sns.palette=&quot;YlOrBr&quot; # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(correlation_data, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}); . Crop yield isn&#39;t highly correlated with any feature and no other features are highly correlated with each other . Plots of crop yield and crop yield against predictor variables . import matplotlib.pyplot as plt import seaborn as sns . df.groupby(&#39;crop&#39;).count() . country year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 . crop . Cassava 2045 | 2045 | 2045 | 2045 | 2045 | 2045 | 2045 | . Maize 4119 | 4119 | 4119 | 4119 | 4119 | 4119 | 4119 | . Plantains and others 556 | 556 | 556 | 556 | 556 | 556 | 556 | . Potatoes 4275 | 4275 | 4275 | 4275 | 4275 | 4275 | 4275 | . Rice, paddy 3388 | 3388 | 3388 | 3388 | 3388 | 3388 | 3388 | . Sorghum 3037 | 3037 | 3037 | 3037 | 3037 | 3037 | 3037 | . Soybeans 3223 | 3223 | 3223 | 3223 | 3223 | 3223 | 3223 | . Sweet potatoes 2890 | 2890 | 2890 | 2890 | 2890 | 2890 | 2890 | . Wheat 3855 | 3855 | 3855 | 3855 | 3855 | 3855 | 3855 | . Yams 847 | 847 | 847 | 847 | 847 | 847 | 847 | . df.boxplot(column=[&#39;hg/ha_yield&#39;], by=[&#39;crop&#39;],rot = 45); . df.boxplot(column=[&#39;hg/ha_yield&#39;], by=[&#39;year&#39;], rot = 45); . Look at distribution and relationship . sns.jointplot(data = df, x=&#39;average_rain_fall_mm_per_year&#39;,y=&quot;hg/ha_yield&quot;); . sns.jointplot(data = df, x=&#39;pesticides_tonnes&#39;,y=&quot;hg/ha_yield&quot;); . sns.jointplot(data = df, x=&#39;avg_temp&#39;,y=&quot;hg/ha_yield&quot;); . sns.jointplot(data = df, x=&#39;co2&#39;,y=&quot;hg/ha_yield&quot;); . 3. Preprocessing . Preprocess data so that (i) categorical variables are numeric and (ii) data is scaled so that features have the same level of magnitudes . from sklearn.preprocessing import OneHotEncoder #(i) df_onehot = pd.get_dummies(df, columns=[&#39;year&#39;,&#39;country&#39;,&#39;crop&#39;]) df_onehot.head() . hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 year_1990 year_1991 year_1992 year_1993 year_1994 ... crop_Cassava crop_Maize crop_Plantains and others crop_Potatoes crop_Rice, paddy crop_Sorghum crop_Soybeans crop_Sweet potatoes crop_Wheat crop_Yams . 0 36613 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 66667 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 23333 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 3 12500 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 4 7000 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 139 columns . x = df_onehot.loc[:, df_onehot.columns != &#39;hg/ha_yield&#39;] # identify response variable y = df[&#39;hg/ha_yield&#39;] . 4. Machine learning . Split features and target data into training and test sets . from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42) #stratification not needed here because we&#39;re not working with groups/classes . Set up function that takes a model as input and calculates R^2 (coefficient of determination) . from sklearn.metrics import r2_score def compare_models(model): model_name = model.__class__.__name__ fit=model.fit(x_train,y_train) y_pred=fit.predict(x_test) r2=r2_score(y_test,y_pred) return([model_name,r2]) . Load in regression models and compare them . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingRegressor # list of models and their parameters models = [ RandomForestRegressor(random_state=0), DecisionTreeRegressor(random_state=0), HistGradientBoostingRegressor(random_state=0) ] . model_train=list(map(compare_models,models)) . print(*model_train, sep = &quot; n&quot;) # asterick &quot;unpacks&quot; output without any comma and brackets . [&#39;RandomForestRegressor&#39;, 0.9861581410126641] [&#39;DecisionTreeRegressor&#39;, 0.9762972871074108] [&#39;HistGradientBoostingRegressor&#39;, 0.9554203899220515] . The Random Forest Regression has the highests R^2 of 98.6%, so next is to optimize the model by determining the appropriate depth to minimize root mean square error . from sklearn.model_selection import cross_val_score depth_range = range(1, 30) depth_scores = [] for d in depth_range: rf_regressor = RandomForestRegressor(max_depth = d) score = cross_val_score(rf_regressor, x_train, y_train, cv=5, scoring=&#39;neg_root_mean_squared_error&#39;) depth_scores.append(score.mean()) . plt.scatter(depth_range, depth_scores) plt.xlabel(&#39;Value of Max Depth for RF Classifier&#39;) plt.ylabel(&#39;RMSE&#39;); . The MSE levels out around a depth of 25, so we rerun the model with that parameter . rf_regressor = RandomForestRegressor(max_depth=15) rf_regressor.fit(x_train, y_train) y_pred = rf_regressor.predict(x_test) . from sklearn import metrics print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 5954.513772343013 Mean Squared Error: 149957445.51220965 Root Mean Squared Error: 12245.711310994133 . plt.subplots(figsize=(8,8)) sns.scatterplot(x=y_test,y=y_pred, alpha=0.5) plt.xlabel(&#39;Actual values&#39;,fontsize=16, weight=&#39;bold&#39;); plt.ylabel(&#39;Predicted values&#39;, fontsize=16,weight=&#39;bold&#39;); plt.title(&#39;Actual vs. Predicted&#39;, fontsize=18,weight=&#39;bold&#39;) plt.tick_params(axis=&#39;both&#39;,labelsize=14); . varimp= {&#39;imp&#39;:rf_regressor.feature_importances_,&#39;names&#39;:df_onehot.columns[df_onehot.columns!=&quot;hg/ha_yield&quot;]} . plt.subplots(figsize=(10,20)) impdf=pd.DataFrame.from_dict(varimp) impdf.sort_values(ascending=False,by=[&quot;imp&quot;],inplace=True) impdf=varimpdf.dropna() sns.barplot(x=&quot;imp&quot;,y=&quot;names&quot;,palette=&quot;vlag&quot;,data=impdf,orient=&quot;h&quot;); . Results . plt.subplots(figsize=(20,10)) impdf=impdf.dropna() impdf=impdf.nlargest(5, &#39;imp&#39;) sns.barplot(x=&quot;imp&quot;,y=&quot;names&quot;,palette=&quot;vlag&quot;,data=impdf,orient=&quot;h&quot;) plt.xlabel(&#39;Importance&#39;,fontsize=16, weight=&#39;bold&#39;); plt.ylabel(&#39;Variable&#39;, fontsize=16,weight=&#39;bold&#39;); plt.title(&#39;Top 5 most important variables in predicting crop yield&#39;, fontsize=18,weight=&#39;bold&#39;) plt.tick_params(axis=&#39;both&#39;,labelsize=14); . First, no two variables were highly correlated. The r values ranged from -0.11 (crop yield and average temperature) to 0.31 (average temperature and annual rainfall). While it is evident that crop yield is not highly correlated with any of the predictor variables, this also demonstrates that there is not multicollinearity amongst the predictor variables that could lead to skewed or misleading results in the modeling approach (although, tree and ensemble methods can generally handle highly correlated predictor variables). The model’s mean absolute error was 5,954, the mean standard error was 149,957,445, while the root mean standard error was 12,245. The error values are high, but there is also a large range crop yield (50 – 501,412 hg/ha). Importantly, the predicted values have a strong correlation with actual values without being overfit. The random forest regression determined the five more important variables in predicting crop yield were: potatoes, CO2, cassava, sweet potatoes, and average temperature. . Discussion . The model may be limited by poor model performance possibly due to the variability between countries in addition to the variable between crops influencing crop yield values. The crop type being potatoes, cassava, or sweet potatoes were three of the most important predictor variables. These crops also had the highest average yields, while potatoes were also the most reported crop. These data characteristics may be influencing their importance in the model. Future analyses may consider modeling on a per-country or per-crop basis to better determine how environmental variables influence crop yields. . Overall, this project highlights the importance of two environmental factors that are key influencers in crop yields: CO2 and average temperature. Moreover, these environmental factors are predicted to vary significantly under future climate change. Given that crop production is tied to these factors, climate change is likely to have dramatic impacts on our global food supply. While global food insecurity is intensifying under country conflicts, the COVID-19 pandemic and its associated supply chain bottlenecks, labor shortages, and economic hardships reducing the general purchasing power of individuals, climate change may further exacerbate these concerns. .",
            "url": "https://cmiller504.github.io/csx4501_projectblog/2022/09/01/_FinalProjectBlog_CMiller.html",
            "relUrl": "/2022/09/01/_FinalProjectBlog_CMiller.html",
            "date": " • Sep 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://cmiller504.github.io/csx4501_projectblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://cmiller504.github.io/csx4501_projectblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://cmiller504.github.io/csx4501_projectblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cmiller504.github.io/csx4501_projectblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
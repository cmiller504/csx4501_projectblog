{
  
    
        "post0": {
            "title": "Environmental predictors of global food supply",
            "content": "Introduction . Climate change is predicted to have major impacts on crop production given that crop yields are dependent on environmental factors such as temperature, precipitation, and carbon dioxide (CO2). Understanding worldwide crop yield is central to addressing food security challenges and reducing the impacts of climate change. Thus, the goals of this project are to (i) create a model of crop yield based on relevant predictor variables and (ii) determine which features are the most important in predicting crop yield. . Methods . Data . I compiled a dataset with 28,235 records for the following variables: crop yield (hg/ha), country, year, rainfall (mm/year), average temperature, pesticide usage (tonnes), and CO2 concentrations (ppm).Data was collected from Kaggle (crop yield: https://www.kaggle.com/datasets/patelris/crop-yield-prediction-dataset; CO2: https://www.kaggle.com/datasets/yoannboyere/co2-ghg-emissionsdata) where crop yield and pesticides were sourced from The Food and Agriculture Organization of the United Nations (FAO, http://www.fao.org/home/en/), rainfall and temperature were sourced from World Bank Data (https://data.worldbank.org/), CO2 was sourced from OurWorldInData (https://ourworldindata.org/co2-and-other-greenhouse-gas-emissions). This dataset includes the topmost consumed crops worldwide: cassava, maize, potatoes, rice (paddy), sorghum, soybeans, sweet potatoes, wheat, yams, and plantains and others. There are records for 101 different countries for years 1990 through 2013. . 1. Importing . import pandas as pd import numpy as np . Load in all of the datasets we will use . yielddf = pd.read_csv(&#39;data/yield_df.csv&#39;) co2df = pd.read_csv(&#39;data/owid-co2-data.csv&#39;) . 2. Cleaning, organizing, and merging . yielddf.drop([&#39;Unnamed: 0&#39;],axis=1,inplace=True) . Area Item Year hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp . 0 Albania | Maize | 1990 | 36613 | 1485.0 | 121.00 | 16.37 | . 1 Albania | Potatoes | 1990 | 66667 | 1485.0 | 121.00 | 16.37 | . 2 Albania | Rice, paddy | 1990 | 23333 | 1485.0 | 121.00 | 16.37 | . 3 Albania | Sorghum | 1990 | 12500 | 1485.0 | 121.00 | 16.37 | . 4 Albania | Soybeans | 1990 | 7000 | 1485.0 | 121.00 | 16.37 | . ... ... | ... | ... | ... | ... | ... | ... | . 28237 Zimbabwe | Rice, paddy | 2013 | 22581 | 657.0 | 2550.07 | 19.76 | . 28238 Zimbabwe | Sorghum | 2013 | 3066 | 657.0 | 2550.07 | 19.76 | . 28239 Zimbabwe | Soybeans | 2013 | 13142 | 657.0 | 2550.07 | 19.76 | . 28240 Zimbabwe | Sweet potatoes | 2013 | 22222 | 657.0 | 2550.07 | 19.76 | . 28241 Zimbabwe | Wheat | 2013 | 22888 | 657.0 | 2550.07 | 19.76 | . 28242 rows × 7 columns . co2df = co2df[[&#39;year&#39;,&#39;country&#39;, &#39;co2&#39;]] . yielddf.rename(columns={&quot;Area&quot;: &quot;country&quot;}, inplace = True) yielddf.rename(columns={&quot;Year&quot;: &quot;year&quot;}, inplace = True) yielddf.rename(columns={&quot;Item&quot;: &quot;crop&quot;}, inplace = True) . df = pd.merge(yielddf, co2df, on=[&#39;year&#39;,&#39;country&#39;]) . nan_rows = df[df.isna().any(axis=1)] . df = df.dropna() . df[&#39;year&#39;] = df[&#39;year&#39;].astype(str) . Analyses . First, to explore potential linear relationships, I tested for correlations between pairs of numerical variables using the Pearson correlation coefficient (r). Next, I considered three supervised machine learning regression algorithms to predict the continuous-valued attribute of crop yield based on the values of several predictor variables: decision tree regression, random forest regression, and histogram-based gradient boosting regression. The histogram-based gradient boosting regression was used instead of a standard gradient boosting regression, because it is recommended for larger datasets (number of samples greater than 10,000). These regression tree and ensemble methods are generally advantageous for finding complex non-linear relationships in data. Since these are non-parametric approaches, they do not require normalized data. . Before applying these machine learning algorithms, data need to be preprocessed by encoding the categorical variables (crop type, year, and country) into numerical features. To do so, I used the scikitlearn OneHotEncoder function which reconfigures categorical features into binary features by “one-hot” encoding, such that if a feature is represented by a column, in that it receives a 1, otherwise, it receives a 0. Data was then split into training and testing sets, 80% and 20% respectively, in order to evaluate model performance. Model performance of the three algorithms were compared by calculating their R² value (the coefficient of determination), which is a statistical measure that determines the proportion of variance in the response variable that can be explained by the predictor variables. This tells us how well the data fit for each of the regression models (the goodness of fit). The random forest regression had the highest R2 of 98.6%, while decision tree regression and histogram gradient boosting regression had 97.6% and 95.5%, respectively. While these are all high R2 values, I selected the random forest regression for the final model. Once the best performing algorithm was determined, I evaluated a range of tree depths between 1 and 30 to determine the appropriate depth to minimize the root mean squared error while also avoiding overfitting the model. More specifically, I graphed the range of depths against their corresponding model’s root mean squared error and selected the value of depth that corresponds to the point in the curve of graphed points where the value of root mean squared error levels out. The model was fit with a depth of 25. I evaluated with model with the following metrics. Finally, I used the model to determine the top five most important variables in predicting crop yield. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. . 3. Plots and basic stats . Any correlations? . df.select_dtypes(include=[np.number]).corr() . hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 . hg/ha_yield 1.000000 | 0.000778 | 0.063992 | -0.114738 | 0.147793 | . average_rain_fall_mm_per_year 0.000778 | 1.000000 | 0.180847 | 0.313180 | -0.024053 | . pesticides_tonnes 0.063992 | 0.180847 | 1.000000 | 0.030996 | 0.270587 | . avg_temp -0.114738 | 0.313180 | 0.030996 | 1.000000 | 0.088063 | . co2 0.147793 | -0.024053 | 0.270587 | 0.088063 | 1.000000 | . import matplotlib.pyplot as plt import seaborn as sns correlation_data=df.select_dtypes(include=[np.number]).corr() mask = np.zeros_like(correlation_data) mask[np.triu_indices_from(mask)] = True f, ax = plt.subplots(figsize=(11, 9)) # Generate a custom diverging colormap cmap = sns.palette=&quot;YlOrBr&quot; # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(correlation_data, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}); . Crop yield isn&#39;t highly correlated with any feature and no other features are highly correlated with each other . Plots of crop yield and crop yield against predictor variables . import matplotlib.pyplot as plt import seaborn as sns . df.boxplot(column=[&#39;hg/ha_yield&#39;], by=[&#39;crop&#39;],rot = 45); . df.boxplot(column=[&#39;hg/ha_yield&#39;], by=[&#39;year&#39;], rot = 45); . Look at distribution and relationship . sns.jointplot(data = df, x=&#39;average_rain_fall_mm_per_year&#39;,y=&quot;hg/ha_yield&quot;); . sns.jointplot(data = df, x=&#39;pesticides_tonnes&#39;,y=&quot;hg/ha_yield&quot;); . sns.jointplot(data = df, x=&#39;avg_temp&#39;,y=&quot;hg/ha_yield&quot;); . sns.jointplot(data = df, x=&#39;co2&#39;,y=&quot;hg/ha_yield&quot;); . 3. Preprocessing . Preprocess data so that categorical variables are numeric . from sklearn.preprocessing import OneHotEncoder df_onehot = pd.get_dummies(df, columns=[&#39;year&#39;,&#39;country&#39;,&#39;crop&#39;]) df_onehot.head() . hg/ha_yield average_rain_fall_mm_per_year pesticides_tonnes avg_temp co2 year_1990 year_1991 year_1992 year_1993 year_1994 ... crop_Cassava crop_Maize crop_Plantains and others crop_Potatoes crop_Rice, paddy crop_Sorghum crop_Soybeans crop_Sweet potatoes crop_Wheat crop_Yams . 0 36613 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 66667 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 23333 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 3 12500 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 4 7000 | 1485.0 | 121.0 | 16.37 | 5.445 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 139 columns . x = df_onehot.loc[:, df_onehot.columns != &#39;hg/ha_yield&#39;] # identify response variable y = df[&#39;hg/ha_yield&#39;] . 4. Machine learning . Split features and target data into training and test sets . from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42) #stratification not needed here because we&#39;re not working with groups/classes . Set up function that takes a model as input and calculates R^2 (coefficient of determination) . from sklearn.metrics import r2_score def compare_models(model): model_name = model.__class__.__name__ fit=model.fit(x_train,y_train) y_pred=fit.predict(x_test) r2=r2_score(y_test,y_pred) return([model_name,r2]) . Load in regression models and compare them . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingRegressor # list of models and their parameters models = [ RandomForestRegressor(random_state=0), DecisionTreeRegressor(random_state=0), HistGradientBoostingRegressor(random_state=0) ] . model_train=list(map(compare_models,models)) . print(*model_train, sep = &quot; n&quot;) # asterick &quot;unpacks&quot; output without any comma and brackets . [&#39;RandomForestRegressor&#39;, 0.9861581410126641] [&#39;DecisionTreeRegressor&#39;, 0.9762972871074108] [&#39;HistGradientBoostingRegressor&#39;, 0.9554203899220515] . The Random Forest Regression has the highests R^2 of 98.6%, so next is to optimize the model by determining the appropriate depth to minimize root mean square error . from sklearn.model_selection import cross_val_score depth_range = range(1, 30) depth_scores = [] for d in depth_range: rf_regressor = RandomForestRegressor(max_depth = d) score = cross_val_score(rf_regressor, x_train, y_train, cv=5, scoring=&#39;neg_root_mean_squared_error&#39;) depth_scores.append(score.mean()) . plt.scatter(depth_range, depth_scores) plt.xlabel(&#39;Value of Max Depth for RF Classifier&#39;) plt.ylabel(&#39;RMSE&#39;); . The MSE levels out around a depth of 25, so we rerun the model with that parameter . rf_regressor = RandomForestRegressor(max_depth=15) rf_regressor.fit(x_train, y_train) y_pred = rf_regressor.predict(x_test) . from sklearn import metrics print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 5954.513772343013 Mean Squared Error: 149957445.51220965 Root Mean Squared Error: 12245.711310994133 . plt.subplots(figsize=(8,8)) sns.scatterplot(x=y_test,y=y_pred, alpha=0.5) plt.xlabel(&#39;Actual values&#39;,fontsize=16, weight=&#39;bold&#39;); plt.ylabel(&#39;Predicted values&#39;, fontsize=16,weight=&#39;bold&#39;); plt.title(&#39;Actual vs. Predicted&#39;, fontsize=18,weight=&#39;bold&#39;) plt.tick_params(axis=&#39;both&#39;,labelsize=14); . varimp= {&#39;imp&#39;:rf_regressor.feature_importances_,&#39;names&#39;:df_onehot.columns[df_onehot.columns!=&quot;hg/ha_yield&quot;]} . plt.subplots(figsize=(10,20)) impdf=pd.DataFrame.from_dict(varimp) impdf.sort_values(ascending=False,by=[&quot;imp&quot;],inplace=True) impdf=varimpdf.dropna() sns.barplot(x=&quot;imp&quot;,y=&quot;names&quot;,palette=&quot;vlag&quot;,data=impdf,orient=&quot;h&quot;); . Results . plt.subplots(figsize=(20,10)) impdf=impdf.dropna() impdf=impdf.nlargest(5, &#39;imp&#39;) sns.barplot(x=&quot;imp&quot;,y=&quot;names&quot;,palette=&quot;vlag&quot;,data=impdf,orient=&quot;h&quot;) plt.xlabel(&#39;Importance&#39;,fontsize=16, weight=&#39;bold&#39;); plt.ylabel(&#39;Variable&#39;, fontsize=16,weight=&#39;bold&#39;); plt.title(&#39;Top 5 most important variables in predicting crop yield&#39;, fontsize=18,weight=&#39;bold&#39;) plt.tick_params(axis=&#39;both&#39;,labelsize=14); . First, no two variables were highly correlated. The r values ranged from -0.11 (crop yield and average temperature) to 0.31 (average temperature and annual rainfall). While it is evident that crop yield is not highly correlated with any of the predictor variables, this also demonstrates that there is not multicollinearity amongst the predictor variables that could lead to skewed or misleading results in the modeling approach (although, tree and ensemble methods can generally handle highly correlated predictor variables). The model’s mean absolute error was 5,954, the mean standard error was 149,957,445, while the root mean standard error was 12,245. The error values are high, but there is also a large range crop yield (50 – 501,412 hg/ha). Importantly, the predicted values have a strong correlation with actual values without being overfit. The random forest regression determined the five more important variables in predicting crop yield were: potatoes, CO2, cassava, sweet potatoes, and average temperature. . Discussion . The model may be limited by poor model performance possibly due to the variability between countries in addition to the variable between crops influencing crop yield values. The crop type being potatoes, cassava, or sweet potatoes were three of the most important predictor variables. These crops also had the highest average yields, while potatoes were also the most reported crop. These data characteristics may be influencing their importance in the model. Future analyses may consider modeling on a per-country or per-crop basis to better determine how environmental variables influence crop yields. . Overall, this project highlights the importance of two environmental factors that are key influencers in crop yields: CO2 and average temperature. Moreover, these environmental factors are predicted to vary significantly under future climate change. Given that crop production is tied to these factors, climate change is likely to have dramatic impacts on our global food supply. While global food insecurity is intensifying under country conflicts, the COVID-19 pandemic and its associated supply chain bottlenecks, labor shortages, and economic hardships reducing the general purchasing power of individuals, climate change may further exacerbate these concerns. .",
            "url": "https://cmiller504.github.io/csx4501_projectblog/fastpages/jupyter/2022/09/02/FinalProject_CMiller.html",
            "relUrl": "/fastpages/jupyter/2022/09/02/FinalProject_CMiller.html",
            "date": " • Sep 2, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://cmiller504.github.io/csx4501_projectblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cmiller504.github.io/csx4501_projectblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}